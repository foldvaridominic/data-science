{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sirs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je0JI_ncVL-H",
        "colab_type": "text"
      },
      "source": [
        "Single Image Super-Resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDMzJqh63xl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "#from keras.layers import Lambda\n",
        "from math import ceil, floor\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adadelta\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from skimage import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfO6ZnCQVGay",
        "colab_type": "text"
      },
      "source": [
        "Some constants and hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwLq3E6L3-d8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = '/content/train/'\n",
        "val_dir = '/content/val'\n",
        "\n",
        "filters = 256\n",
        "kernel_size = 3\n",
        "strides = 1\n",
        "res_blocks = 1\n",
        "subpix_scale = 2\n",
        "\n",
        "batch_size = 8\n",
        "inner_batch = 2\n",
        "epochs = 15\n",
        "num_training_samples = 200\n",
        "num_validation_samples = 100\n",
        "\n",
        "input_size = 300\n",
        "crop_length_l = HR_size = 256\n",
        "crop_length_s = LR_size = 64\n",
        "\n",
        "scale_fact = 4\n",
        "img_depth = 3\n",
        "overlap = 16\n",
        "\n",
        "img_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oswXbK7CWm9v",
        "colab_type": "text"
      },
      "source": [
        "Subpixel convolution layer and crop function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5rsy1uP4hIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SubpixelConv2D(input_shape, scale=4):\n",
        "    def subpixel_shape(input_shape):\n",
        "        return input_shape[0], input_shape[1] * scale, input_shape[2] * scale, input_shape[3] // scale**2\n",
        "    def subpixel(x):\n",
        "        return tf.depth_to_space(x, scale)\n",
        "    return Lambda(subpixel, output_shape=subpixel_shape)\n",
        "\n",
        "def random_crop(img, random_crop_size):\n",
        "    height, width = img.shape[:2]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    return img[y:(y+dy), x:(x+dx)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BsMXZKcWfxo",
        "colab_type": "text"
      },
      "source": [
        "Batch generator and model initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB1uPINL4iP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(batches, crop_length_l=256, crop_length_s=64):\n",
        "    while True:\n",
        "        batch =  next(batches)\n",
        "        len_batch = batch.shape[0]\n",
        "        for i in range(len_batch):\n",
        "            new_batch_x = np.zeros((inner_batch,  crop_length_s, crop_length_s, 3))\n",
        "            new_batch_y = np.zeros((inner_batch,  crop_length_l, crop_length_l, 3))\n",
        "            for j in range(inner_batch):\n",
        "                cropped = random_crop(batch[i], (crop_length_l, crop_length_l))\n",
        "                resized = cv2.resize(cropped, (crop_length_s, crop_length_s))\n",
        "                new_batch_x[j] = resized\n",
        "                new_batch_y[j] = cropped\n",
        "            yield (new_batch_x, new_batch_y)\n",
        "\n",
        "def get_model():\n",
        "    # Head\n",
        "    input_ = Input(name='input', shape=(crop_length_s, crop_length_s, img_depth))\n",
        "    conv0 = Conv2D(filters, kernel_size, strides=strides, padding='same')(input_)\n",
        "\n",
        "    # Body\n",
        "    res = Conv2D(filters, kernel_size, strides=strides, padding='same')(conv0)\n",
        "    act = ReLU()(res)\n",
        "    res = Conv2D(filters, kernel_size, strides=strides, padding='same')(act)\n",
        "    res_rec = Add()([conv0, res])\n",
        "    for i in range(res_blocks):\n",
        "        res1 = Conv2D(filters, kernel_size, strides=strides, padding='same')(res_rec)\n",
        "        act  = ReLU()(res1)\n",
        "        res2 = Conv2D(filters, kernel_size, strides=strides, padding='same')(act)\n",
        "        res_rec = Add()([res_rec, res2])\n",
        "    conv = Conv2D(filters, kernel_size, strides=strides, padding='same')(res_rec)\n",
        "    add  = Add()([conv0, conv])\n",
        "\n",
        "    # Tail\n",
        "    conv = Conv2D(filters, kernel_size, strides=strides, padding='same')(add)\n",
        "    act  = ReLU()(conv)\n",
        "    up   = SubpixelConv2D(input_shape=act.shape, scale=subpix_scale)(act)\n",
        "    conv = Conv2D(filters, kernel_size, strides=strides, padding='same')(up)\n",
        "    act  = ReLU()(conv)\n",
        "    up   = SubpixelConv2D(input_shape=act.shape, scale=subpix_scale)(act)\n",
        "    output = Conv2D(name='output', filters=3, kernel_size=1, strides=1, padding='same')(up)\n",
        "    model = Model(inputs=input_, outputs=output)\n",
        "    optimizer = Adadelta(lr=1.0, rho=0.95, decay=0.0)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics = ['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAWd8bSFW8n-",
        "colab_type": "text"
      },
      "source": [
        "Initialize training and validation batches and the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNoYY2fl5AE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = img_datagen.flow_from_directory(train_dir, target_size=(input_size, input_size), batch_size=batch_size, class_mode=None)\n",
        "val_generator = img_datagen.flow_from_directory(val_dir, target_size=(input_size, input_size), batch_size=batch_size, class_mode=None)\n",
        "\n",
        "train_batches = batch_generator(train_generator) \n",
        "val_batches = batch_generator(val_generator)\n",
        "\n",
        "model = get_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJoomY0ZXA9T",
        "colab_type": "text"
      },
      "source": [
        "Train, save or load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSE009_75CTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_batches, steps_per_epoch=ceil(inner_batch * num_training_samples / batch_size), epochs=epochs, validation_data=val_batches, validation_steps=ceil(inner_batch * num_validation_samples / batch_size),)\n",
        "\n",
        "#model.load_weights('weights.h5')\n",
        "\n",
        "model.save_weights('weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adf-OzQfacXj",
        "colab_type": "text"
      },
      "source": [
        "Predict images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCU1dFWkmk-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(image_name):\n",
        "  image = io.imread(image_name)[:, :]\n",
        "  SR, crops = predict_crops(image)\n",
        "  reconstructed_image = reconstruct(SR, crops)\n",
        "  height_pad = (reconstructed_image.shape[0]-image.shape[0])/2\n",
        "  width_pad = (reconstructed_image.shape[1]-image.shape[1])/2\n",
        "  reconstructed_image = reconstructed_image[ceil(height_pad): reconstructed_image.shape[0]-floor(height_pad), ceil(width_pad): reconstructed_image.shape[1]-floor(width_pad)]\n",
        "  return image, reconstructed_image, cv2.resize(image, (image.shape[1]//scale_fact, image.shape[0]//scale_fact))\n",
        "  \n",
        "def predict_crops(image):\n",
        "    height, width = image.shape[:2]\n",
        "    height_pad = HR_size-(height%(HR_size-overlap))\n",
        "    width_pad = HR_size-(width%(HR_size-overlap))\n",
        "    pad_width = ((ceil(height_pad/2), floor(height_pad/2)), (ceil(width_pad/2), floor(width_pad/2)), (0, 0))\n",
        "    padded_image = np.pad(image, pad_width, 'constant')\n",
        "    crops = seq_crop(padded_image)\n",
        "    SR= []\n",
        "    for crop_row in crops:\n",
        "        for crop in tqdm(crop_row):\n",
        "            LR_image = cv2.resize(crop, (LR_size, LR_size))\n",
        "            SR_image = model.predict(np.expand_dims(LR_image, 0))[0]\n",
        "            SR_image = SR_image[overlap//2:HR_size-overlap//2, overlap//2:HR_size-overlap//2]\n",
        "            SR.append(SR_image)\n",
        "    return SR, crops\n",
        "\n",
        "def seq_crop(img):\n",
        "    sub_images = []\n",
        "    j, shifted_height = 0, 0\n",
        "    while shifted_height < (img.shape[0] - HR_size):\n",
        "        horizontal = []\n",
        "        shifted_height = j * (HR_size - overlap)\n",
        "        i, shifted_width = 0, 0\n",
        "        while shifted_width < (img.shape[1] - HR_size):\n",
        "            shifted_width = i * (HR_size - overlap)\n",
        "            horizontal.append(crop_precise(img, shifted_width, shifted_height, HR_size, HR_size))\n",
        "            i += 1\n",
        "        sub_images.append(horizontal)\n",
        "        j += 1\n",
        "    return sub_images\n",
        "\n",
        "def crop_precise(img, coord_x, coord_y, width_length, height_length):\n",
        "    tmp_img = img[coord_y:coord_y + height_length, coord_x:coord_x + width_length]\n",
        "    return float_im(tmp_img)\n",
        "\n",
        "def reconstruct(predictions, crops):\n",
        "    def nest(data, template):\n",
        "        data = iter(data)\n",
        "        return [[next(data) for _ in row] for row in template]\n",
        "    predictions = nest(predictions, crops)\n",
        "    H = np.cumsum([x[0].shape[0] for x in predictions])\n",
        "    W = np.cumsum([x.shape[1] for x in predictions[0]])\n",
        "    D = predictions[0][0]\n",
        "    recon = np.empty((H[-1], W[-1], D.shape[2]), D.dtype)\n",
        "    for rd, rs in zip(np.split(recon, H[:-1], 0), predictions):\n",
        "        for d, s in zip(np.split(rd, W[:-1], 1), rs):\n",
        "            d[...] = s\n",
        "    tmp_overlap = overlap * (scale_fact - 1)\n",
        "    return recon[tmp_overlap:recon.shape[0]-tmp_overlap, tmp_overlap:recon.shape[1]-tmp_overlap]\n",
        "  \n",
        "def float_im(img):\n",
        "    return np.divide(img, 255.)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}